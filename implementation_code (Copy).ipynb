{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "spelling_model = gensim.models.KeyedVectors.load_word2vec_format('spellingCorrection_model/bnword2vec.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import resnet34\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = resnet34.resnet34().to(device)\n",
    "\n",
    "# Load the trained weights\n",
    "model.load_state_dict(torch.load('recongnition_model/char_level_trained_model_128x224_shoroborno_again.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def convert_images(input_folder, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    files = os.listdir(input_folder)\n",
    "\n",
    "    for file in files:\n",
    "        if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', 'bmp')):\n",
    "            input_path = os.path.join(input_folder, file)\n",
    "\n",
    "            original_image = Image.open(input_path)\n",
    "\n",
    "            converted_image = original_image.convert('RGB')\n",
    "            converted_image = converted_image.resize((640, 640))\n",
    "\n",
    "            output_path = os.path.join(output_folder, os.path.splitext(file)[0] + '.jpg')\n",
    "            converted_image.save(output_path, 'JPEG', quality=95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect mode=predict model=detection_model/weights2/last_medium.pt conf=0.5 source='handw_s_demo_test' save_txt=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using labels from: runs/detect/predict14/labels\n"
     ]
    }
   ],
   "source": [
    "#gets the most recent labels folder to use the latest results. potential improvement might be getting the directory name from the yolo model\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "def get_folder_number(folder_path):\n",
    "    match = re.search(r'predict(\\d+)$', folder_path)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return 0 \n",
    "\n",
    "predict_folders = glob.glob(\"runs/detect/predict*\")\n",
    "\n",
    "if predict_folders:\n",
    "    latest_folder = sorted(predict_folders, key=get_folder_number)[-1]\n",
    "else:\n",
    "    latest_folder = \"runs/detect/predict\"\n",
    "\n",
    "\n",
    "# Path to the directory containing the txt files\n",
    "labels_directory = f\"{latest_folder}/labels\"\n",
    "print(f\"Using labels from: {labels_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'handw_s_demo_annotations.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "\n",
    "csv_file_path = 'handw_s_demo_annotations.csv'\n",
    "\n",
    "# Initialize a defaultdict to store lists for each image_id\n",
    "data_dict = defaultdict(lambda: {'x_center': [], 'y_center': [], 'width': [], 'height': []})\n",
    "\n",
    "# Iterate over txt files in the directory\n",
    "for filename in os.listdir(labels_directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        # Extract image_id from the filename\n",
    "        image_id = filename.split('.txt')[0]\n",
    "\n",
    "        # Read the content of the txt file\n",
    "        with open(os.path.join(labels_directory, filename), 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        # Parse each line and aggregate information\n",
    "        for line in lines:\n",
    "            parts = line.strip().split(' ')\n",
    "            _, x_center, y_center, width, height = map(float, parts)\n",
    "\n",
    "            # Append information to lists in the defaultdict\n",
    "            data_dict[image_id]['x_center'].append(x_center)\n",
    "            data_dict[image_id]['y_center'].append(y_center)\n",
    "            data_dict[image_id]['width'].append(width)\n",
    "            data_dict[image_id]['height'].append(height)\n",
    "\n",
    "# Write data to CSV file\n",
    "with open(csv_file_path, mode='w', newline='') as csv_file:\n",
    "    fieldnames = ['image_id', 'x_center', 'y_center', 'width', 'height']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "    # Write header\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Write data rows\n",
    "    for image_id, data in data_dict.items():\n",
    "        writer.writerow({\n",
    "            'image_id': image_id,\n",
    "            'x_center': data['x_center'],\n",
    "            'y_center': data['y_center'],\n",
    "            'width': data['width'],\n",
    "            'height': data['height']\n",
    "        })\n",
    "\n",
    "print(f\"CSV file '{csv_file_path}' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted data saved to handw_s_demo_sorted_annotations.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "# Specify the paths to your input and output CSV files\n",
    "input_csv_path = 'handw_s_demo_annotations.csv'\n",
    "output_csv_path = 'handw_s_demo_sorted_annotations.csv'\n",
    "\n",
    "# Open the input CSV file\n",
    "with open(input_csv_path, 'r') as input_csv_file:\n",
    "    # Create a CSV reader object with header\n",
    "    csv_reader = csv.DictReader(input_csv_file)\n",
    "\n",
    "    # Extract and sort data\n",
    "    rows = []\n",
    "    for row in csv_reader:\n",
    "        image_id = row['image_id']\n",
    "        x_c = ast.literal_eval(row['x_center'])\n",
    "        y_c = ast.literal_eval(row['y_center'])\n",
    "        w = ast.literal_eval(row['width'])\n",
    "        h = ast.literal_eval(row['height'])\n",
    "\n",
    "        # Sort the data based on x_center\n",
    "        sorted_bundle = sorted(zip(x_c, y_c, w, h), key=lambda bundle: bundle[0])\n",
    "        sorted_x, sorted_y, sorted_w, sorted_h = zip(*sorted_bundle)\n",
    "\n",
    "        # Create a new row with sorted values\n",
    "        new_row = {\n",
    "            'image_id': image_id,\n",
    "            'x_center': str(sorted_x),\n",
    "            'y_center': str(sorted_y),\n",
    "            'width': str(sorted_w),\n",
    "            'height': str(sorted_h)\n",
    "        }\n",
    "\n",
    "        # Append the new row to the list\n",
    "        rows.append(new_row)\n",
    "\n",
    "# Write the sorted data to a new CSV file\n",
    "fieldnames = ['image_id', 'x_center', 'y_center', 'width', 'height']\n",
    "\n",
    "with open(output_csv_path, 'w', newline='') as output_csv_file:\n",
    "    csv_writer = csv.DictWriter(output_csv_file, fieldnames=fieldnames)\n",
    "\n",
    "    # Write the header\n",
    "    csv_writer.writeheader()\n",
    "\n",
    "    # Write the sorted rows\n",
    "    csv_writer.writerows(rows)\n",
    "\n",
    "print(f\"Sorted data saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coordinates(x_center, y_center, width, height):\n",
    "    # Calculate half-width and half-height\n",
    "    half_width = width / 2\n",
    "    half_height = height / 2\n",
    "\n",
    "    # Calculate the four coordinates\n",
    "    x1 = x_center - half_width\n",
    "    y1 = y_center - half_height\n",
    "    x3 = x_center + half_width\n",
    "    y3 = y_center + half_height\n",
    "\n",
    "    return x1, y1, x3, y3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "def image_patch(path, wid=640, hei=640, csv_path='handw_s_demo_sorted_annotations.csv', image=None):\n",
    "    # Split the path using '/' and get the last part\n",
    "    filename = os.path.split(path)[-1]\n",
    "\n",
    "    if(filename.split('.')[0].isdigit()):\n",
    "        image_id_to_find = ''.join(c for c in filename if c.isdigit())\n",
    "    else:\n",
    "        image_id_to_find = filename.split('_')[0]\n",
    "\n",
    "    # Read the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Find the rows with the specified image_id\n",
    "    matching_rows = df[df['image_id'] == int(image_id_to_find)]\n",
    "\n",
    "    # Initialize patches list\n",
    "    patches = []\n",
    "\n",
    "    # Check if there are any matching rows\n",
    "    if not matching_rows.empty:\n",
    "        # Extract the first matching row\n",
    "        row = matching_rows.iloc[0]\n",
    "\n",
    "        # Parse the string representations of lists\n",
    "        x_c = ast.literal_eval(row['x_center'])\n",
    "        y_c = ast.literal_eval(row['y_center'])\n",
    "        w = ast.literal_eval(row['width'])\n",
    "        h = ast.literal_eval(row['height'])\n",
    "\n",
    "        # Convert normalized coordinates to pixel coordinates\n",
    "        x_c = [x * wid for x in x_c]\n",
    "        y_c = [y * hei for y in y_c]\n",
    "        w = [width * wid for width in w]\n",
    "        h = [height * hei for height in h]\n",
    "\n",
    "        # Open the image if not provided\n",
    "        if image is None:\n",
    "            from PIL import Image\n",
    "            image = Image.open(path)\n",
    "        \n",
    "        img_np = np.array(image)\n",
    "\n",
    "        # Extract patches for each bounding box\n",
    "        for i in range(len(x_c)):\n",
    "            # Coordinates for each character\n",
    "            xc, yc, width, height = x_c[i], y_c[i], w[i], h[i]\n",
    "\n",
    "            # Calculate other coordinates\n",
    "            x1, y1, x3, y3 = calculate_coordinates(xc, yc, width, height)\n",
    "\n",
    "            x1, y1, x3, y3 = int(x1), int(y1), int(x3), int(y3)\n",
    "\n",
    "            # Ensure coordinates are within image bounds\n",
    "            x1 = max(0, x1)\n",
    "            y1 = max(0, y1)\n",
    "            x3 = min(img_np.shape[1], x3)\n",
    "            y3 = min(img_np.shape[0], y3)\n",
    "\n",
    "            # Extract the patch\n",
    "            patch = img_np[y1:y3, x1:x3]\n",
    "            patches.append(patch)\n",
    "    else:\n",
    "        print(f\"No matching rows found for image_id {image_id_to_find}\")\n",
    "        print(df)\n",
    "        # Return empty list when no matches are found\n",
    "        \n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "grapheme_root_components = ['০','১','২','৩','৪','৫','৬','৭','৮','৯','ং', 'ঃ', 'অ', 'আ', 'ই', 'ঈ', 'উ', 'ঊ', 'ঋ', 'এ', 'ঐ', 'ও', 'ঔ', 'ক', 'ক্ক', 'ক্ট', 'ক্ত', 'ক্ল', 'ক্ষ', 'ক্ষ্ণ', 'ক্ষ্ম', 'ক্স', 'খ', 'গ', 'গ্ধ', 'গ্ন', 'গ্ব', 'গ্ম', 'গ্ল', 'ঘ', 'ঘ্ন', 'ঙ', 'ঙ্ক', 'ঙ্ক্ত', 'ঙ্ক্ষ', 'ঙ্খ', 'ঙ্গ', 'ঙ্ঘ', 'চ', 'চ্চ', 'চ্ছ', 'চ্ছ্ব', 'ছ', 'জ', 'জ্জ', 'জ্জ্ব', 'জ্ঞ', 'জ্ব', 'ঝ', 'ঞ', 'ঞ্চ', 'ঞ্ছ', 'ঞ্জ', 'ট', 'ট্ট', 'ঠ', 'ড', 'ড্ড', 'ঢ', 'ণ', 'ণ্ট', 'ণ্ঠ', 'ণ্ড', 'ণ্ণ', 'ত', 'ত্ত', 'ত্ত্ব', 'ত্থ', 'ত্ন', 'ত্ব', 'ত্ম', 'থ', 'দ', 'দ্ঘ', 'দ্দ', 'দ্ধ', 'দ্ব', 'দ্ভ', 'দ্ম', 'ধ', 'ধ্ব', 'ন', 'ন্জ', 'ন্ট', 'ন্ঠ', 'ন্ড', 'ন্ত', 'ন্ত্ব', 'ন্থ', 'ন্দ', 'ন্দ্ব', 'ন্ধ', 'ন্ন', 'ন্ব', 'ন্ম', 'ন্স', 'প', 'প্ট', 'প্ত', 'প্ন', 'প্প', 'প্ল', 'প্স', 'ফ', 'ফ্ট', 'ফ্ফ', 'ফ্ল', 'ব', 'ব্জ', 'ব্দ', 'ব্ধ', 'ব্ব', 'ব্ল', 'ভ', 'ভ্ল', 'ম', 'ম্ন', 'ম্প', 'ম্ব', 'ম্ভ', 'ম্ম', 'ম্ল', 'য', 'র', 'ল', 'ল্ক', 'ল্গ', 'ল্ট', 'ল্ড', 'ল্প', 'ল্ব', 'ল্ম', 'ল্ল', 'শ', 'শ্চ', 'শ্ন', 'শ্ব', 'শ্ম', 'শ্ল', 'ষ', 'ষ্ক', 'ষ্ট', 'ষ্ঠ', 'ষ্ণ', 'ষ্প', 'ষ্ফ', 'ষ্ম', 'স', 'স্ক', 'স্ট', 'স্ত', 'স্থ', 'স্ন', 'স্প', 'স্ফ', 'স্ব', 'স্ম', 'স্ল', 'স্স', 'হ', 'হ্ন', 'হ্ব', 'হ্ম', 'হ্ল', 'ৎ', 'ড়', 'ঢ়', 'য়']\n",
    "vowel_diacritic_components = ['0', 'া', 'ি', 'ী', 'ু', 'ূ', 'ৃ', 'ে', 'ৈ', 'ো', 'ৌ']\n",
    "consonant_diacritic_components = ['0', 'ঁ', 'র্', 'র্য', '্য', '্র', '্র্য', 'র্্র']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "\n",
    "def detect(patches):\n",
    "    predicted = []\n",
    "    for patch in patches:\n",
    "        img = Image.fromarray(patch)\n",
    "        enhancer = ImageEnhance.Contrast(img)\n",
    "        img_enhanced = enhancer.enhance(2.0)\n",
    "        img_inv = ImageOps.invert(img_enhanced)\n",
    "        img_inv = img_inv.resize((224,128)).convert('RGB')\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5,), (0.5,)) \n",
    "                ])\n",
    "\n",
    "        img_ten = transform(img_inv)\n",
    "\n",
    "        input_image = img_ten.unsqueeze(0) \n",
    "\n",
    "        input_image = input_image.to(device)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        # Forward pass to get model predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_image)\n",
    "\n",
    "        # Find the index of the maximum probability for each class\n",
    "        predicted_grapheme_idx = torch.argmax(outputs[0]).item()\n",
    "        predicted_vowel_idx = torch.argmax(outputs[1]).item()\n",
    "        predicted_consonant_idx = torch.argmax(outputs[2]).item()\n",
    "\n",
    "        predicted_grapheme_root = torch.tensor(predicted_grapheme_idx)\n",
    "        predicted_vowel_diacritic = torch.tensor(predicted_vowel_idx)\n",
    "        predicted_consonant_diacritic = torch.tensor(predicted_consonant_idx)\n",
    "\n",
    "        pred_char = []\n",
    "\n",
    "        if(predicted_vowel_idx != 0 and predicted_consonant_idx != 0):\n",
    "                if(predicted_consonant_idx == 2):\n",
    "                    pred_char.append( consonant_diacritic_components[predicted_consonant_idx] +  grapheme_root_components[predicted_grapheme_idx] + vowel_diacritic_components[predicted_vowel_idx] )\n",
    "                elif(predicted_consonant_idx == 1):\n",
    "                    pred_char.append(grapheme_root_components[predicted_grapheme_idx] + vowel_diacritic_components[predicted_vowel_idx] + consonant_diacritic_components[predicted_consonant_idx])\n",
    "                else:\n",
    "                    pred_char.append(grapheme_root_components[predicted_grapheme_idx] + consonant_diacritic_components[predicted_consonant_idx] + vowel_diacritic_components[predicted_vowel_idx])\n",
    "        \n",
    "        elif(predicted_vowel_idx == 0 and predicted_consonant_idx == 0):\n",
    "            pred_char.append(grapheme_root_components[predicted_grapheme_idx])\n",
    "\n",
    "        else:\n",
    "            if(predicted_vowel_idx == 0):\n",
    "                if(predicted_consonant_idx == 2):\n",
    "                    pred_char.append( consonant_diacritic_components[predicted_consonant_idx] +  grapheme_root_components[predicted_grapheme_idx] )\n",
    "                else:\n",
    "                    pred_char.append(grapheme_root_components[predicted_grapheme_idx] + consonant_diacritic_components[predicted_consonant_idx])\n",
    "            else:\n",
    "                pred_char.append(grapheme_root_components[predicted_grapheme_idx] + vowel_diacritic_components[predicted_vowel_idx])\n",
    "        \n",
    "        predicted.append(pred_char)  \n",
    "              \n",
    "    predicted = ''.join([char for sublist in predicted for char in sublist])    \n",
    "    print(predicted)\n",
    "    correct_word(predicted)\n",
    "    plt.imshow(image,cmap='gray')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def correct_word(word):\n",
    "    if word in spelling_model:\n",
    "        similar_words = spelling_model.similar_by_word(word)\n",
    "        print(f\"Words similar to '{word}':{word}\")\n",
    "    else:\n",
    "        vocab = spelling_model.index_to_key\n",
    "        closest_word = difflib.get_close_matches(word, vocab, n=1)\n",
    "        if closest_word:\n",
    "            closest_word = closest_word[0]\n",
    "            print(f\"The closest word is '{closest_word}'\")\n",
    "        else:\n",
    "            print(f\"No similar word found in the model's vocabulary or through difflib.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'অংশ':অংশ\n"
     ]
    }
   ],
   "source": [
    "correct_word('অংশ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path to the demo folder\n",
    "demo_folder_path = 'handw_s_demo'\n",
    "\n",
    "# Get a list of all files in the demo folder\n",
    "all_files = os.listdir(demo_folder_path)\n",
    "\n",
    "# Randomly select 9 files from the list\n",
    "selected_files = random.sample(all_files, 9)\n",
    "\n",
    "# Iterate over the selected files\n",
    "for filename in selected_files:\n",
    "    # Construct the full path to the image\n",
    "    image_path = os.path.join(demo_folder_path, filename)\n",
    "\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    width, height = image.size\n",
    "    print(width, height)\n",
    "    # Process image patches (assuming image_patch and detect functions are defined elsewhere)\n",
    "    patches = image_patch(image_path, width, height, 'handw_s_demo_sorted_annotations.csv')\n",
    "    detect(patches=patches)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
